import argparse
import numpy as np
import torch

def setup_parser():
    parser = argparse.ArgumentParser(description='Extraction Attack: Single Evaluation Run')
    ### Arguments to control Prompt Creation for Evaluation
    parser.add_argument('--evalfile', required=True, type=str, help='File with Indices and Location of Sentences to Evaluate')
    parser.add_argument('--promptlen', default=50, type=int, help='Length of Prompt')
    parser.add_argument('--complen', default=50, type=int, help='Length of Completion')
    parser.add_argument('--prompttype', default='standard', type=str, help='Prompt Formatting Before Feeding it to the Model')
    parser.add_argument('--instructions', default=None, type=str, help='Any Additional Instructions Added Before the Prompt')

    ### LLM Hyperparameters for Generation
    parser.add_argument('--temperature', default=0., type=float, help='Temperature for LLM Generation')
    parser.add_argument('--beamwidth', default=1, type=int, help='Beam Search Width for LLM Generation')
    parser.add_argument('--maxtokens', default=50, type=int, help='Max tokens for LLM Generation')
    parser.add_argument('--sampling', action='store_true', help='Do sampling for LLM Generation')

    ### Model Configuration
    parser.add_argument('--modelsize', default='pythia-1.4b', type=str, help='Model Size')
    parser.add_argument('--modelstep', default='step100000', type=str, help='Training Step for Checkpoint')

    ### Scoring Method for Evaluation
    parser.add_argument('--scoring', default='exact', type=str, help='Scoring Method for Evaluation')

    ### Other Arguments
    parser.add_argument('--batchsize', default=1, type=int, help='Evaluation Batch Size')
    return parser

def get_filename(args, args_ignore=None):
    args_dict = vars(args)
    filename = ''
    for ele in args_dict:
        if args_ignore is not None and ele in args_ignore:
            continue
        filename += ele + '-' + str(args_dict[ele]) + '__'
    
    return filename[:-2] + '.log'

def prompt_scoring(orig_ids, gen_ids, scoring='exact'):
    orig_ids, gen_ids = np.array(orig_ids), np.array(gen_ids)
    ## TODO: It records the perfect match for now, but we want to record how much matches later.
    if scoring=='exact':
        outscores = np.all(orig_ids==gen_ids, axis=-1)
        return outscores
    elif scoring=='length':
        outbools = orig_ids!=gen_ids
        outboolspad = np.concatenate((outbools, np.ones((outbools.shape[0], 1))), axis=-1)
        outlengths = np.argmax(outboolspad, axis=-1)
        return outlengths
    else:
        raise NotImplementedError("Evaluation scoring method %s is not implemented" % scoring)
    
def get_instruction_ids(instruction_format):
    if instruction_format is None:
        return None
    ### TODO: The instructions flag will only contain a marker for what kind of instruction needs to be added.
    ### First, we will need to use that to get the actual long form instruction string. 
    ### And then we need to tokenize it before sending it to the prompt dataset creator.